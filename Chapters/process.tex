\chapter{Process}

%********************************** %First Section  **************************************
\section{Diagnostic} 

The idea of exploring the topic of testing in a Dynamics 365 for Finance and Operation environment was firstly introduced as a possible solution to compensate the lack of a proper internal methodology to follow during the development of a project in order maintain high standards of quality and reliability for a given project trough a structured testing procedure. 
At the beginning of this work we explained how the purpose of the Diagnostic phase was the creation of a project plan that would allow us to form an initial determination of the project requirements together with their scope and the amount of time to dedicate to each activity. This allowed us to better understand how many resources needed to be allocated for the completion of the project and gave us a tool to monitor to which degree we were able to respect the predefined timeline. In this section we will not provide further information regarding the structure of the plan (section 1.4.2) and the defined requirements (Chapter 2).

A secondary activity was setting up the environment that would later be used to access the development tools. The company account was created and given development access to the Trade+ project on Azure DevOps. A dev virtual machine was also set up and the Visual Studio Environment was connected to the project using Visual Studio Team Explorer. Relevant links were provided in order to access the Dynamics Learning Portal, Lifecycle Services, Microsoft Developers Network, the company Intranet and the Trade+ project portal.

%********************************** %Second Section  **************************************
\section{Analysis} 

The identification of the activities that had to be covered by the developed test cases revolved around finding a good balance between the skills that could be acquired during the training activities and the usefulness of the chosen processes. As mentioned before (Section 1.4.2) we settled on five business processes that had to be analyzed in order to have a satisfactory output (both in terms of coverage and documentation) at the end of the project. Said processes, together with the steps that compose them, read as follows:

\begin{itemize}
    \item \textbf{Purchase Order}. Creation of new license plates to identify the purchased goods, creation of a new PO (with vendor and arrival site and warehouse), specification of items number and quantity, order confirmation, specification of order lines, baydoor registration, creation of product receipt (with id), posting of product receipt, invoice creation, invoice posting.
    \item \textbf{Production Order}. Creation of PO (with item number, delivery date and quantity), validation of PO, update of BOM (bill of materials) and production route, estimation and cost management, scheduling/release/start of production, reporting of production as finished (with publishing of journal entry). 
    \item \textbf{Transfer Order}. Creation of TO (with origin and target warehouse), specification of items number and quantity, inventory reservation, items picking from origin warehouse, items shipping to target warehouse, receival confirmation.
    \item \textbf{Sales Order}. Moving items from receival area to storage (transfer journal), creation of a new SO (with customer account and origin site and warehouse), specification of items number and quantity, inventory reservation, release to warehouse (creation of work item), validation and completion of work, creation of shipping load, shipment.  
    \item \textbf{Return Order}. Creation of RO (with customer account, return site and warehouse and return reason code), specification of items number and quantity, specification of order lines, baydoor registration.
\end{itemize}

After this initial definition we also decided that it would be appropriate (as a secondary step) not only to test the business processes in isolation (\textit{feature testing}), but also as single coherent workflow (\textit{integration testing}) in which the various tasks were interconnected with one another [Figure \ref{fig:processesWorkflow}] and where a given set of purchased (and produced) goods moved through all the stages of the covered processes.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{Images/workflow.pdf}
	\caption{Processes workflow}
	\label{fig:processesWorkflow}
\end{figure}

The Analysis phase was also characterized by a series of training activities (mentioned in section 1.3) that were organized in order to give a satisfactory introduction to the world of ERP applications and that naturally had a particular focus on the Dynamics suite. The "Introductory lectures for new hires" were carried out with regularity by the company and aimed to form new team members to the internal practices, policies and tools. During the first weeks the following lectures were organized: Introduction to Dynamics, look and feel of Dynamics 365, sales and logistics, production, development process overview and introduction to Trade+. On the Dynamics Learning Portal we followed the set of lectures called "Development, Extensions and Deployment for Microsoft Dynamics 365 for Finance and Operations" (Exam MB6-894) that covered: The understanding of Dynamics 365 for Finance and Operations architecture (application stack, cloud components, server architecture, layers) and development environment (Visual Studio and Lifecycle Services), development of new elements (creation and management of data types, tables and labels), the X++ programming language, user interface and security and component extension. Additional (independent) lectures were also followed on the topics of: System architecture and technologies (80765BE), cloud deployment (81144AE), advanced security (80929BE) and DevOps testing/practices/principles (Steven Borg, Sam Guckenheimer).

%********************************** %Third Section  **************************************
\section{Design} 

Dynamics 365 for Finance and Operations provides a easy-to-use tool (the \textit{Task Recorder}) to record business processes and download them as .xml or .axtr files. The tool is integrated in the browser client and, when activated, records every user interaction with the system creating a step-by-step list of the performed actions. Each step appears to the user as a human readable, high level description of a single action (e.g. In the Warehouse field, type '6') and, once the recording file is downloaded, corresponds to a portion of the XML code that provides the system with information on the operation that has to be performed on the GUI. The recording process can be interrupted and resumed at any point in time and every created step can be commented (to provide further information to the users that will reproduce the interested recording), modified or deleted. A downloaded recording can be saved in the Lifecycle Service portal as a future reference guide (.axtr), attached to a DevOps work item (.axtr or .xml), played back and edited by another user in his Dynamics environment (.axtr or .xml) and converted into a test case (.xml). The playback functionality is particularly useful to guide developers when a bug (that triggers after a series of specific actions) needs to be fixed. Because of this reason the recordings are usually created by consultants and attached to Azure Boards work items. Said items (as mentioned in section 4.2) are assigned to developers that can later download and use the attached recording file to emulate the interested behavior on their personal environment in order to better understand the problem without having to only rely on a written description.

After an initial period, necessary in order to get accustomed with the tool, we started to plan the recording activities. A major focus during this phase was the reduction of the steps needed to cover the interested business processes. This activity was of particular importance in order to reduce the amount of code that a given recording would later generate when converted into a coded test case. A shorter test case is generally easier to maintain, expand and modify. Furthermore, superfluous steps would have increased the likelihood of unexpected errors and behaviors. Other than reducing the total number of steps, we also made sure that each recording could be played back on any machine running the same version of Dynamics 365 for Finance and Operations, independently from the underlying status of the database at the start of the execution. This meant encompassing all the activities related with the creation of the needed data at the beginning (or during) the interested business process (e.g. If a license plate needs to be used, create it before using it). Some information (like the predefined selection in a drop-down menu) is saved locally by Dynamics 365 for Finance and Operations and depends on how the user has previously interacted with the system. This peculiarity had to be taken into consideration during the recording activities in order to create file that could be played back (and later executed) on any machine (e.g. make sure that each item in a form is explicitly defined during the recording). 

Regarding the business processes covered in our work we decided to create a single recording for each activity, ending the Design phase with five .xml files that could later be utilized to create the foundation for our test cases.

%********************************** %Fourth Section  **************************************
\section{Development} 

This section covers all the Development tasks that were carried out during the course of our work. We decided to organize it into three subsections corresponding to the different macro-activities of this phase (mentioned in section 1.4.2). The first subsection describes the \textit{creation} of the project that hosted our code and the \textit{translation} of the task recordings into coded test cases. The second one provides information regarding how said test have been \textit{expanded} in order to fulfill the testing requirements of the application. Finally, the last subsection describes the process of \textit{integrating} the test cases with the nightly builds.

\subsection{Creation}

As mentioned before (section 4.1), the handling of Dynamics 365 for Finance and Operations elements is done via a project that needs to be tied to a single model. Because of this reason, at the beginning of this phase, we decided to create a new model to contain all of our test classes. This new model (called \textit{TradePlusTest}) was generated in his own package, hence following the \textit{Extension} technique (discussed in section 4.1) and referenced the ones belonging to the application stack (Application Platform, Application Foundation and Application Suite). This was required in order for the test classes to interact with the application elements (e.g forms, tables, menu items, etc.). Another reference to the \textit{TestEssentials} model was created in order to have access to the various methods and classes part of the SysTest Framework (section 4.3) needed to write functioning test classes.

Once the project was created, the five .xml recordings containing the instructions needed to cover our business processes were imported in the Visual Studio environment via the dedicated software utility that converted each recording into an executable X++ test case. During this initial stage the coded test cases were left mostly unmodified from their original state and a successful test run merely indicated that the system was able to correctly perform all the actions needed to complete the interested business process. As mentioned before (section 4.3), generated test cases are not created with any assert statement. Despite this fact it is possible to add them later given the fact that test cases created from task recordings still extend the \textit{SysTestAssert} class that provides the assert statement functionalities. As a first development activity we decided to verify if the actions performed in the various business processes during a test run produced the expected results on the application database. This task included running SQL queries at the beginning and end of each test in order to check (using the assert statements) if the data corresponding to the interested items was correctly created or modified during the business process execution. This activity, while still technically not expanding the scope of each test case, allowed us to obtain an initial outlook on how to interact with the generated X++ code.

\subsection{Expansion}

For expanding our tests we focused on verifying the behavior of the system in a variety of edge cases. That is, interactions that did not strictly comply to the ones expected during a normal execution of the business processes. This approach had the goal of proving the ability of the system of correctly reacting to an array of unexpected scenarios, not only by being able to resume the temporarily compromised workflow, but also by providing exhaustive information messages to the end user. Furthermore, having a set of "expanded" tests (and documenting the tasks related to this expansion) could provide a point of reference for the future testing activities. 
During the course of our work, we realized that implementing assert statements to expand the scope of our test cases meant analyzing all the actions and sub-tasks that were part of our business processes and recognizing and asserting all (or at least most of) the \textit{successful} and \textit{unsuccessful} scenarios that could derive from said interactions with the system.
The analysis of unsuccessful scenarios was of particular importance for us given the fact we not only needed to verify that the client was able to handle these situations correctly (i.e. still being usable after an unexpected event occurred), but also that the appropriate error or warning message was prompted during the process. 
This last task revealed itself to be a particularly challenging activity given the fact that it required finding a balance between specificity and future proofness (e.g. what happens if an error message is changed after an application update?). Because of this reason we decided to handle the verification of prompted messages by identifying the presence of specific \textit{keywords} that indicated that the correct issue was addressed by the application without having to rely on a (much less flexible) hard-coded identification of the whole message. This approach allowed us to verify the usability of Trade+ in the context of the tested processes without having to constantly refactor our test code as changes were brought to the application by new updates.

The identification and analysis of sub-activities in our business processes was of central importance in order to plan the expansion of the generated test cases. This task allowed us to better understand potential points of weakness in the application and to adapt our development activities accordingly. The Analysis section of this chapter provided a good overlook on the various steps composing the investigated business processes. In this subsection, we decided to follow the same type of structural description in order to introduce the analyzed edge scenarios used to expand our test cases:

\begin{itemize}
    \item \textbf{Purchase Order}. Creation of LP using already existing and invalid ids. New PO with non existing vendor/site/warehouse and with invalid combinations of said values. Addition of non existing or not available items. Insertion of out-of-range or invalid quantities. Missing or invalid values (site/location/license plate) during order registration. Creation of receipt without or with already existing id. Creation of invoice without or with already existing id, posting of invoice without matching process. 
    \item \textbf{Production Order}. Creation of PO using non existing item number and invalid quantity. Past or invalid delivery date during validation. Modification of bill of materials lines (components/ingredients) and versions. Invalid quantity in production route. Past or invalid scheduling date. Invalid good/error quantity and error cause values.
    \item \textbf{Transfer Order}. Creation of TO with non existing/invalid/same origin and target warehouse. Addition of non existing or not available item ids. Insertion of out-of-range or invalid quantities. Reservation of unavailable items. Missing or invalid values (location/license plate) during item picking.
    \item \textbf{Sales Order}. Transfer journal with invalid or non existing origin/target site and warehouse and item quantity. Transfer Journal posting without validation. New SO with non existing customer account/site/warehouse and with invalid combinations of said values. Insertion of out-of-range or invalid quantities. Non existing user id for warehouse work validation, completion and shipment confirmation.
    \item \textbf{Return Order}. New RO with non existing customer/site/warehouse and with invalid combinations of said values. Invalid insertion of return reason code. Insertion of out-of-range or invalid quantities. Missing or invalid license plate.
\end{itemize}

Given the fact that some business processes share the same (or very similar) subroutines, it stands to reason that that certain portion of different test cases will be expanded in the same way. At the same time, there are actions that are repeated multiple times inside the same process (e.g. if we create a purchase order for two items, we will need to repeat the activities related to each item multiple times). Both of this situations bring redundancy in the developed code that, subsequently, creates longer (and less manageable) test cases. Because of this reason, we decided to define a new shared class, called \textit{SharedTests}, that would allow multiple business processes to handle this repetitions with a simple method call. In this newly created class each method represented either a \textit{subroutine} (a series of actions performed on the application interface) or a test \textit{expansion} (setting up an edge case, getting the error/warning message, asserting). As mentioned before (section 4.3), each generated test has a \textit{setupData} method that assigns values to the used variables. We decided to create an additional method, \textit{setupSharedTestsData}, called at the end of \textit{setupData} in order to pass the needed values for a given test case to the \textit{sharedTests} class.

Up until this point we have dealt with our business processes in isolation (component testing). This required us to generate starting data at the beginning of some test cases in order to have the necessary prerequisites for a successful execution. The previously mentioned rollback mechanism (section 4.3) made it somewhat difficult to merge the various processes in a single workflow (integration testing), given the fact that the changes made by a given business process were lost before the execution of the following one. Because of this reason, we decided to further investigate the structure of the SysTest framework in order to find a solution that would better suit our needs. Initially, we considered merging all the covered processes in a single class. Each business process would be represented by a single method in the newly created class and a "master method" would call the various business processes one after the other. This technique would have prevented the data rollback given the fact that all the tests were executed from a single method. This approach was ultimately discarded given the impact it would have had on the granularity of our work. 
Further examination into the (at the time poorly documented) framework revealed that the creation of a \textit{Test Suite} was a valid alternative to overcome the data issue. Because of this reason, we implemented the functionalities provided by the \textit{SysTestSuite} class by adding all the business processes (together with the \textit{SharedTests} class) in a suite [Figure \ref{fig:TestSuite}] that was recognized as a single entity by the framework and therefore allowed us to create an homogeneous workflow between the developed test cases.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.6]{Images/TestSuite.pdf}
	\caption{Structure of the test suite}
	\label{fig:TestSuite}
\end{figure}

\subsection{Build Integration}

The next step in the development process revolved around the integration of the developed test cases in the nightly builds. The build process can be monitored and extended in the Azure Pipelines section of the Azure DevOps environment. In Dynamics 365 for Finance and Operation it is handled by a build VM that contains build agent(s) (that run the computational operations of the build process and that are connected directly to the interested Azure DevOps project), controller (that distributes the build workload to all the available agents), process template (where the build steps are defined) and definition (that specifies what you want to build, when a build should start, where the build output should be sent). The tests that need to be added to the build process are specified in the build definition and are executed after the build process has proven to be successful. Test execution is organized into three activities: test setup, test execution and test end. The Trade+ project code is subjected to nightly builds that are executed every 24 hours and that can naturally support the integration of test cases via the Azure DevOps application. We decided to begin the integration process by adding a newly generated test case (without any added code or assert statement) in order to verify that the addition of the testing steps in the build pipeline would function without any issues for our project. Then, we decided to purposefully insert a failing assert statement in order to verify the reaction of the system. At this point we were able to verify, trough the builds recap feature provided by Azure Pipelines, that the last build "partially succeeded". By expanding the interested entry we could establish the pipeline step causing the problem (test execution) and explore the details regarding the specific failure (name of the test case, error line, error message). Finally, after having verified that each aspect of the integration process worked as intended, we decided to add the developed test suite to the build pipeline effectively enabling the regression testing aspect of the project and ending the development phase.

%********************************** %Fifth Section  **************************************
\section{Deployment}

The integration of the developed test cases in the build process marked the end of the development phase. The focus of the next activities (part of the deployment phase) was related with the process of sharing to the company the knowledge acquired during the various tasks carried out to complete our project. This technical and procedural know-how was presented through a series of final meetings that were focused on analyzing the individual phases executed to create and integrate (in the nightly builds) the set of developed test cases. Given the fact that said tasks have already been described in detail during the course of this chapter, we will not provide further information on this matter inside this subsection. Another topic that was discussed during this "end project" meetings was a general evaluation regarding the resources required (both in terms of time and human capital) to implement the described techniques together with some information regarding various strategies that could be implemented in order to reduce the total amount of resources needed in order to complete a similar set of testing activities. We considered important to communicate that during the course of our work the quality of a an XML recording could greatly influence the time spent refactoring the code of a generated X++ test case. We highlighted how the more good practices (like reducing the total number of steps to complete the business process, planning carefully the whole process before starting the recording activities, etc.) were respected during an XML creation, the least the developer had to work on the initial test code. Refactoring generated code was usually a lengthy and cumbersome process, it was always faster to create a good recording in order to save time during the development phase. We also stressed the importance of reducing as much as possible the amount of code for a given test case. Working with shorter tests was always easier and ideally a fine level of granularity should be seeked in order to increment the control over unsuccessful test executions. We communicated that the activity of splitting a long process into multiple sub-tasks and recognizing subroutines that have already been seen in other processes (in order to execute them in shared class) was also an important technique that allowed us to reduce code redundancy and create more manageable test cases. As a final concept we wanted to communicate to the company that expanding a test case is ultimately (and simply) a matter of adding code that verifies that a given scenario behaves in the expected way under a predefined array of conditions. Starting with a solid knowledge of the application (together with a certain level of expertise regarding the SysTest framework and the X++ language) and clear understanding on what has to be checked in a given business process usually means that this goal is achievable in a fairly limited amount of time (often in a matter of hours when the relevant documentation is available) and that, because of this reason, the  techniques described in this work may represent a viable resource for the company in order to increment the quality e reliability of their software. The conclusion of this meetings indicated that the deployment phase was over and that all the relevant information were correctly communicated to the company.

%********************************** %Sixth Section  **************************************
\section{Operation (Future Work)} 

The various activities related to this project were considered concluded after the know-how was passed through the previously mentioned meetings. Every task performed by the company to support and further expand the applied testing techniques falls under the operational phase and can be considered part of the future work. In this subsection we will briefly explore what could require attention in the future in order to maintain the already developed test cases. It is important to mention how Dynamics 365 for Finance and Operations falls under the Microsoft Modern Lifecycle Policy which mandates users to update their application within 30 days after the official release of a new software version. Microsoft refuses to investigate issues or troubleshoot implementations that do not comply with the imposed deadline and that, because of this reason, are considered outdated. Knowing this fact it is easy to understand the importance of having test cases that work with a certain degree of flexibility regarding slight changes to the underlying software infrastructure. We already mentioned that techniques such as the generalization in the identification process of error/warning messages through the use of specific keywords will certainly help in this regard. Despite this fact it is fairly unreasonable to aim at the creation of a test case that will not be eventually affected by a software update. Because of this reason the refactoring of already developed test code will certainly be one of the main activities if the company will decide to further implement the testing techniques mentioned in this work. Fortunately the nightly build integration and, in a more general sense, the application of regression testing techniques together with the monitoring tools provided by Microsoft (i.e. Azure DevOps) allows the developer to identify the point of failure with a high degree of accuracy as soon as problems arise during the test execution. Other than the maintenance of test cases related to the already covered business processes the company will need to expand the developed suite in order to enlarge the scope of the performed testing activities and to increase the relevance of this technique in order to obtain an improved development and maintenance process on the application. The particulars...